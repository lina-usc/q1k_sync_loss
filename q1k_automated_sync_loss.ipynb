{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate automated session reports for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import mne_bids\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "import os\n",
    "import papermill as pm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "from nbformat import read\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select task parameters and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = \"/project/def-emayada/q1k/experimental/HSJ/\"\n",
    "pyll_path = \"derivatives/pylossless/\"\n",
    "sync_loss_path = \"derivatives/sync_loss/\"\n",
    "code_path = \"code/q1k_sync_loss/\"\n",
    "out_path = \"\"\n",
    "#out_seg_path = \"epoch_fif_files/\"\n",
    "#out_avg_path = \"erp_fif_files/\"\n",
    "html_reports_path = \"session_reports/\"\n",
    "task_id = \"VEP\"\n",
    "#subject_id = 'Q1K_HSJ_100123_F1'\n",
    "run_id = '1'\n",
    "session_id = '01'\n",
    "site_code = 'HSJ' #'MHC' or 'HSJ'...\n",
    "#sourcedata_path = \"sourcedata/\" \n",
    "#html_reports_path = \"session_reports/\" + task_id_in  + '/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see which task you would like to make reports for \n",
    "print(task_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find existing *.edf files \n",
    "pattern = project_path + pyll_path + sync_loss_path + '**/eeg/*' + task_id + '*.edf'\n",
    "processed_sessions = glob.glob(pattern, recursive=True)\n",
    "        \n",
    "pattern = project_path + pyll_path + '**/eeg/*' + task_id + '*.edf'\n",
    "file_paths = glob.glob(pattern, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"completed files:\")\n",
    "processed_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Input files:\")\n",
    "file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract details from filename\n",
    "def extract_job_info(filename):\n",
    "    # Define the pattern to capture the required sections\n",
    "    pattern = r\"sub-(.*?)_ses-(.*?)_task-(.*?)_run-(.*?)_eeg\\.edf\"\n",
    "    match = re.match(pattern, filename)\n",
    "\n",
    "    if match:\n",
    "        # Extract the groups from the match\n",
    "        subject_id = match.group(1)\n",
    "        session_id = match.group(2)\n",
    "        task_id = match.group(3)\n",
    "        run_id = match.group(4)\n",
    "        return subject_id, session_id, task_id, run_id\n",
    "    else:\n",
    "        raise ValueError(\"Filename pattern did not match.\")\n",
    "\n",
    "\n",
    "#in_pattern = project_path + pyll_path + '**/eeg/*' + task_id + '*.edf'\n",
    "#in_file_paths = glob.glob(in_pattern, recursive=True)\n",
    "#print(f\"Input files:\")\n",
    "#for file_path in in_file_paths:\n",
    "#    file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "#    print(f\"File Path: {file_path}, File Name: {file_name}\")\n",
    "\n",
    "    \n",
    "## Make sure the output directories exists\n",
    "#if not os.path.exists(project_path + pyll_path + post_path + out_seg_path + task_id_out):\n",
    "#    os.makedirs(project_path + pyll_path + post_path + out_seg_path + task_id_out)\n",
    "#if not os.path.exists(project_path + pyll_path + post_path + out_avg_path + task_id_out):\n",
    "#    os.makedirs(project_path + pyll_path + post_path + out_avg_path + task_id_out)\n",
    "    \n",
    "#out_seg_pattern = project_path + pyll_path + post_path + out_seg_path + task_id_out + '/*' + task_id_out + '*epo.fif'\n",
    "#out_seg_file_paths = glob.glob(out_seg_pattern, recursive=True)\n",
    "#print(f\"existing output epoch files:\")\n",
    "#for file_path in out_seg_file_paths:\n",
    "#    file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "#    print(f\"File Path: {file_path}, File Name: {file_name}\")\n",
    "\n",
    "#out_avg_pattern = project_path + pyll_path + post_path + out_avg_path + task_id_out + '/*' + task_id_out + '*epo.fif'\n",
    "#out_avg_file_paths = glob.glob(out_seg_pattern, recursive=True)\n",
    "#print(f\"existing output avg files:\")\n",
    "#for file_path in out_avg_file_paths:\n",
    "#    file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "#    print(f\"File Path: {file_path}, File Name: {file_name}\")\n",
    "\n",
    "#    #submit_slurm_job(project_path, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate html session reports for all participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glob.glob(project_path + pyll_path + '**/*.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the output directory exists\n",
    "if not os.path.exists(html_reports_path):\n",
    "    os.makedirs(html_reports_path)\n",
    "\n",
    "# Create a list of sessions with errors\n",
    "error_subjects = []\n",
    "\n",
    "#Loop through existing sessions and execute the q1k_generate_individual_reports.ipynb if a session report does not already exist\n",
    "#for file in glob.glob(project_path + pyll_path + \"**/eeg/*\" + task_id + '_*.edf'):\n",
    "for file in file_paths:\n",
    "\n",
    "    file_name = os.path.basename(file)\n",
    "    print('Current data file: ' + file_name)\n",
    "    ## Select anything after the Q1K and before the AEP\n",
    "    ##subject_id = file.split('_')[2]\n",
    "    ##site_part = file.split(\"HSJ_\")\n",
    "    subject_id = file_name.split(\"_\")[0].split(\"-\")[1]\n",
    "    print('Participant ID: ' + subject_id)\n",
    "\n",
    "    #subject_relation = site_part[1].split(\"_\")[1].split(\"/\")[0]\n",
    "    #print('Participant relation: ' + subject_relation)\n",
    "\n",
    "    #subject_id_in = subject_number + \"_\" + subject_relation\n",
    "    #print('Participant ID input: ' + subject_id_in)\n",
    "    #subject_id_out = subject_number.replace('_','').replace('-','') + subject_relation\n",
    "    #print('Participant ID output: ' + subject_id_out)\n",
    "    \n",
    "    # Skip sessions that have already been processed\n",
    "    print(subject_id)\n",
    "    if subject_id in processed_sessions:\n",
    "        print(subject_id + ' has already been processed')\n",
    "        continue    \n",
    "\n",
    "    # Define paths\n",
    "    input_notebook = project_path + pyll_path + sync_loss_path + 'code/q1k_sync_loss/' + 'q1k_sync_loss.ipynb'\n",
    "\n",
    "    ##output_notebook = f'./../../session_reports/{task_id_in}/executed_notebooks/{subject_id}_{task_id_in}_executed.ipynb'\n",
    "    ##output_html = f'./../../session_reports/{task_id_in}/{subject_id}_{session_id}_{task_id_in}.html'\n",
    "    ##output_html = f'{html_reports_dir}/{subject_id}.html'\n",
    "    output_notebook = f'{project_path}{pyll_path}{sync_loss_path}{html_reports_path}executed_notebooks/{subject_id}_{task_id}_executed.ipynb'\n",
    "    print('Output notebook file: ' + output_notebook)\n",
    "    output_html = f'{project_path}{pyll_path}{sync_loss_path}{html_reports_path}{task_id}/{subject_id}_{task_id}.html'\n",
    "    print('Output HTML file: ' + output_html)\n",
    "\n",
    "\n",
    "    #print('Current data file: ' + file)\n",
    "    ## Select anything after the Q1K and before the AEP\n",
    "    ##subject_id = file.split('_')[2]\n",
    "    #site_part = file.split(site_code + \"_\")\n",
    "    #subject_number = site_part[1].split(\"_\")[0]\n",
    "    #print('Participant number: ' + subject_number)\n",
    "    #subject_relation = site_part[1].split(\"_\")[1].split(\"/\")[0]\n",
    "    #print('Participant relation: ' + subject_relation)\n",
    "\n",
    "    #subject_id_in = subject_number + \"_\" + subject_relation\n",
    "    #print('Participant ID input: ' + subject_id_in)\n",
    "    #subject_id_out = subject_number.replace('_','').replace('-','') + subject_relation\n",
    "    #print('Participant ID output: ' + subject_id_out)\n",
    "    \n",
    "    ## Skip sessions that have a session report in the output directory\n",
    "    #print(subject_id_in)\n",
    "    #if subject_id_in in processed_sessions:\n",
    "    #    print(subject_id_in + ' has already been processed')\n",
    "    #    continue    \n",
    "\n",
    "    ## Define paths\n",
    "    #input_notebook = project_path + pyll_path + postproc_path +'q1k_sync_ll_prune.ipynb'\n",
    "    #print('Input notebook: ' + input_notebook)\n",
    "\n",
    "    ## Make sure the directory exists\n",
    "    #if not os.path.exists(f'{project_path}{pyll_path}{postproc_path}session_reports/{task_id}/executed_notebooks/'):\n",
    "    #    os.makedirs(f'{project_path}{init_path}{html_reports_path}{task_id}/executed_notebooks/')\n",
    "    \n",
    "    #output_notebook = f'{project_path}{pyll_path}{postproc_path}{html_reports_path}executed_notebooks/{subject_id_in}_{task_id}_executed.ipynb'\n",
    "    #print('Output notebook file: ' + output_notebook)\n",
    "    #output_html = f'{project_path}{pyll_path}{postproc_path}{html_reports_path}{subject_id_in}_{task_id}.html'\n",
    "    #print('Output HTML file: ' + output_html)\n",
    "\n",
    "    try:\n",
    "        # Execute the notebook\n",
    "        pm.execute_notebook(input_notebook, output_notebook, kernel_name = 'q1k_env', parameters=dict(subject_id=subject_id, task_id=task_id, run_id=run_id, session_id=session_id))\n",
    "\n",
    "        # Convert executed notebook to HTML\n",
    "        html_exporter = HTMLExporter()\n",
    "        html_exporter.exclude_input = True\n",
    "\n",
    "        (body, resources) = html_exporter.from_filename(output_notebook)\n",
    "\n",
    "        # Save HTML output\n",
    "        with open(output_html, 'w', encoding='utf-8') as f:\n",
    "            f.write(body)\n",
    "\n",
    "        print(f\"HTML report saved for {subject_id}.\")\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle the error \n",
    "        error_subjects.append(subject_id)\n",
    "        print(f\"Error while processing {subject_id}: {e}\")\n",
    "\n",
    "# Print out the list of subjects with errors\n",
    "print( \"These subjects have errors: \" + str(error_subjects) + \" and need to be reprocessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "q1k_env",
   "language": "python",
   "name": "q1k_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
